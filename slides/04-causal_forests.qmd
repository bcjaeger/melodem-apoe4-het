---
title: "Causal random forests with `grf`"
subtitle: "MELODEM data workshop"
author: "Byron C. Jaeger, PhD"
institute: "Wake Forest University School of Medicine"
format: 
  revealjs:
    slide-number: true
    footer: Slides available at <https://bcjaeger.github.io/melodem-apoe4-het/>
    include-before-body: header.html
    include-after-body: footer-annotations.html
    theme: [default, tidymodels.scss]
    width: 1280
    height: 720
---

```{r setup, cache=FALSE, include=FALSE}

knitr::opts_chunk$set(echo = FALSE, 
                      message = FALSE,
                      warning = FALSE,
                      dpi = 300,
                      cache = TRUE,
                      fig.height = 7.25,
                      out.width = '100%',
                      fig.align = 'center')


withr::with_dir(
  new = here::here(), code = {
    R.utils::sourceDirectory('R')
    library(tidyverse)
    library(tidymodels)
    library(butcher)
    library(grf)
    targets::tar_load(data_sim_1)
  }
)

```

## Overview

This lecture focuses on the the causal random forest algorithm

- Robinson's residual-on-residual regression

- Causal trees

- Causal random forest

- Causal random survival forest

- Inference with CATE summaries

# Robinson's residual-on-residual regression

## The partially linear model

Suppose

$$
Y_i = \tau W_i + f(X_i) + \varepsilon_i
$$
Assume: 

- $E[\varepsilon_i | X_i, W_i] = 0$

- untreated outcome is given by unknown function $f$, 

- a treatment assignment shifts the outcome by $\tau$.


## How to estimate $\tau$?

Suppose

$$
Y_i = \tau W_i + f(X_i) + \varepsilon_i 
$$

How do we estimate $\tau$ when we do not know $f(X_i)$? 

Define:

\begin{align*}

e(x) &= E[W_i | X_i=x] \,\, \text{(Propensity score)} \\

m(x) &= E[Y_i | X_i = x] = f(x) + \tau e(x) \,\,\,\,\, \text{(Cndl. mean of } Y\text{)}

\end{align*}

## Use propensity and conditional mean

Re-express the partial linear model in terms of $e(x)$ and $m(x)$:

\begin{align*}

Y_i &= \tau W_i + f(X_i) + \varepsilon_i, \,  \\

Y_i - \tau e (x) &= \tau W_i + f(X_i) - \tau e(x) + \varepsilon_i, \, \\

Y_i - f(X_i) - \tau e (x) &= \tau W_i - \tau e (x) + \varepsilon_i, \, \\

Y_i - m(x) &= \tau (W_i - e(x)) + \varepsilon_i, \, \\

\end{align*}

$\tau$ can be estimated with residual-on-residual regression [@robinson1988root]. 

**How?** Plug in flexible estimates of $m(x)$ and $e(x)$

## Re-write as a linear model

More formally, 

$$
\hat{\tau} := \text{lm}\Biggl( Y_i - \hat m^{(-i)}(X_i) \sim W_i - \hat e^{(-i)}(X_i)\Biggr).
$$

- Superscript $^i$ denotes cross-fit estimates [@debiased_ML_Chernozhukov]. 

- Cross-fitting: estimate something, e.g., $e(x)$, using cross-validation. 

- Why? removes bias from over-fitting.

## Example

:::{.column width="45%"}

Suppose $Y_i = \tau W_i + f(X_i) + \epsilon_i$

- $\tau$ is 1/2
- $W_i$ is randomized treatment
- $X_i$ is a continuous covariate
- $f(X_i) = |X_i|$

By defn, $E[W_i] = 1/2$ (why?). 

:::

:::{.column width="53%"}

```{r echo=TRUE}

# Set up for const tau example

set.seed(1)
tau_truth <- 1/2
n <- 1000

# randomized treatment
W <- rbinom(n = n, size = 1, prob = 1/2)

# continuous covariate
X <- rnorm(n = n)

# outcome 
Y <- tau_truth * W + abs(X) + rnorm(n)

data <- data.frame(Y=Y, X=X, W=W)

```

:::

## Example done wrong

:::{.column width="45%"}

First we'll do it the wrong way.

- Fit a **classical** model to estimate conditional mean of $Y$.

- Compute residuals and run Robinson's regression.

- What'd we do wrong?

:::

:::{.column width="53%"}

```{r, echo=TRUE}

library(glue)
library(ggplot2)

fit_cmean <- lm(Y ~ X, data = data)

m_x <- predict(fit_cmean, new_data = data)

resid_y <- Y - m_x
resid_w <- W - 1/2

tau_fit <- lm(resid_y ~ resid_w) 

glue("True tau is {tau_truth}, \\
      estimated tau is {coef(tau_fit)[2]}")


```

:::

## Conditional mean predictions...

```{r, fig.width = 6, fig.height = 6}

mse <- round(mean((m_x - Y)^2), 2)

ggplot(data = data.frame(predicted = m_x, observed = Y)) + 
  aes(x = predicted, y = observed) + 
  geom_point() + 
  geom_smooth() + 
  labs(title = glue("Linear model predictions: MSE = {mse}"))


```

## Example done wrong, take 2

:::{.column width="45%"}

The model for conditional mean was under-specified.

- Fit a **flexible** model to estimate conditional mean of $Y$.

- Compute residuals and run Robinson's regression.

- What'd we do wrong?

:::

:::{.column width="53%"}

```{r, echo=TRUE}

library(aorsf)

fit_cmean <- orsf(Y ~ X, data = data)

m_x <- predict(fit_cmean, new_data = data)

resid_y <- Y - m_x
resid_w <- W - 1/2

tau_fit <- lm(resid_y ~ resid_w) 

glue("True tau is {tau_truth}, \\
      estimated tau is {coef(tau_fit)[2]}")


```

:::

## Example done right

:::{.column width="45%"}

We forgot about cross-fitting!

- Fit a flexible model to estimate conditional mean of $Y$.

- Use **out-of-bag predictions**.

- Compute residuals and run Robinson's regression.

:::

:::{.column width="53%"}

```{r, echo=TRUE}

m_x_oobag <- predict(fit_cmean, oobag = TRUE)

resid_y <- Y - m_x_oobag
resid_w <- W - 1/2

tau_fit <- lm(resid_y ~ resid_w) 

glue("True tau is {tau_truth}, \\
      estimated tau is {coef(tau_fit)[2]}")


```

:::

## Conditional mean predictions

```{r, fig.width=12, fig.height=6}

p1 <- ggplot(data = data.frame(predicted = m_x, 
                               observed = Y)) + 
  aes(x = predicted, y = observed) + 
  geom_point() + 
  geom_smooth()

p2 <- ggplot(data = data.frame(predicted = m_x_oobag, 
                               observed = Y)) + 
  aes(x = predicted, y = observed) + 
  geom_point() + 
  geom_smooth()

library(table.glue)

library(patchwork)

(p1 + labs(title=table_glue("In-bag predictions: MSE = {mean((m_x - Y)^2)}"))) + 
  (p2 + labs(title=table_glue("Out-of-bag predictions: MSE = {mean((m_x_oobag - Y)^2)}"))) + 
  plot_layout(axes = 'collect')

```

# Causal trees

## How to grow causal trees

Causal trees are much like standard decision trees, but they maximize

$$n_L \cdot n_R \cdot (\hat{\tau}_L-\hat{\tau}_R)^2$$

where residual-on-residual regression is used to estimate $\hat{\tau}_L$ and $\hat{\tau}_R$

- `grf` estimates $\hat \tau$ *once* in the parent node and uses "influence functions" to approximate how $\hat\tau$ would change if an observation moved from one child node to the other [@wager2018estimation]. 

- Predictions from leaves are $E[Y|W=1] - E[Y|W=0]$


## How to grow causal trees

Causal trees use "honesty" and "subsampling" [@wager2018estimation]. 

- **Honesty**: Each training observation is used for **one** of the following:

    + Estimate the treatment effect for leaf nodes.

    + Decide splitting values for non-leaf nodes.

- **Subsampling**: While @breiman2001random's random forest uses bootstrap sampling with replacement, the causal random forest samples without replacement. 

# Causal random forest

## Back to the partial linear model

Relaxing the assumption of a constant treatment:

$$
Y_i = \color{red}{\tau(X_i)} W_i + f(X_i) + \varepsilon_i, \, 
$$

where $\color{red}{\tau(X_i)}$ is the conditional average treatment (CATE). If we had a neighborhood $\mathcal{N}(x)$ where $\tau$ was constant, then we could do residual-on-residual regression in the neighborhood:

$$
\hat\tau_i(x) := lm\Biggl( Y_i - \hat m^{(-i)}(X_i) \sim W_i - \hat e^{(-i)}(X_i), \color{red}{w = 1\{X_i \in \mathcal{N}(x) \}}\Biggr),
$$

## Random forest adaptive neighborhoods

Suppose we fit a random forest with $B$ trees to a training set of size $n$, and we compute a prediction $p$ for a new observation $x$:

\begin{equation*}
\begin{split}
& p = \sum_{i=1}^{n} \frac{1}{B} \sum_{b=1}^{B} Y_i \frac{1\{Xi \in L_b(x)\}} {|L_b(x)|}
\end{split}
\end{equation*}

- $L_b(x)$ indicates the leaf node that $x$ falls into for tree $b$

- The inner sum is the mean of outcomes in the same leaf as $x$

- This generalizes to causal random forests (it's easier to write with regression trees).

## Random forest adaptive neighborhoods

Pull $Y_i$ out of the sum that depends on $b$:

\begin{equation*}
\begin{split}
p &= \sum_{i=1}^{n} \frac{1}{B} \sum_{b=1}^{B} Y_i \frac{1\{Xi \in L_b(x)\}} {|L_b(x)|} \\
&= \sum_{i=1}^{n} Y_i \sum_{b=1}^{B} \frac{1\{Xi \in L_b(x)\}} {B \cdot |L_b(x)|} \\
& = \sum_{i=1}^{n} Y_i \color{blue}{\alpha_i(x)},
\end{split}
\end{equation*}

- $\alpha_i(x) \propto$ no. of times observation $i$ lands in the same leaf as $x$

## Plug weights in to `lm`

Instead of defining neighborhood boundaries, weight by similarity:

$$
\hat\tau_i(x) := \text{lm}\Biggl( Y_i - \hat m^{(-i)}(X_i) \sim W_i - \hat e^{(-i)}(X_i), w = \color{blue}{\alpha_i(x)} \Biggr).
$$
This forest-localized version of Robinson's regression, paired with honesty and subsampling, gives asymptotic guarantees for estimation and inference [@wager2018estimation]:

1. Pointwise consistency for the true treatment effect.

2. Asymptotically Gaussian and centered sampling distribution.

## Three main components 

The procedure to estimate $\hat\tau_i$ has three pieces:

$$
\hat\tau_i(x) := \text{lm}\Biggl( Y_i - \color{green}{\hat m^{(-i)}(X_i)} \sim W_i - \color{red}{\hat e^{(-i)}(X_i)}, w = \color{blue}{\alpha_i(x)} \Biggr).
$$

1. $\color{green}{\hat m^{(-i)}(X_i)}$ is a flexible, cross-fit estimate for $E[Y|X]$

2. $\color{red}{\hat e^{(-i)}(X_i)}$ is a flexible, cross-fit estimate for $E[W|X]$

3. $\color{blue}{\alpha_i(x)}$ are the similarity weights from a causal random forest

# Causal random survival forest

## Set up

Assume the survival setting:

\begin{equation}
  Y_i =
    \begin{cases}
      T_i & \text{if } \, T_i \leq C_i \\
      C_i & \text{otherwise}
    \end{cases}
\end{equation}

Where $T_i$ is time to event and ($C_i$) is time to censoring. Define

\begin{equation}
D_i =
    \begin{cases}
      1 & \text{if } \, T_i \leq C_i \\
      0 & \text{otherwise.}
    \end{cases}
\end{equation}

## Observed time versus true time

:::{.column width="39%"}

Event times are obscured by

- censoring

- end of follow-up, i.e., $h$

:::

:::{.column width="60%"}

```{r fig.width=4, fig.height=4, out.width='80%'}

n <- 1e6
failure.time <- rexp(n, rate = 0.1)
censor.time <- runif(n, 0, 12)
Y <- pmin(failure.time, censor.time)
D <- as.integer(failure.time <= censor.time)
cc <- (D==1)

dens.cens <- with(density(failure.time), data.frame(x, y))
dens.obs <- with(density(Y[cc]), data.frame(x, y))
df <- rbind(dens.cens, dens.obs)

gg_data <- df

gg_data$distribution <-
  c(rep("Time to event", 
        nrow(dens.cens)), 
    rep("Observed time", 
        nrow(dens.obs)))

gg_data$distribution <-
  factor(gg_data$distribution,
         levels = c("Time to event", "Observed time"))

ggplot(gg_data) +
  aes(x = x,
      y = y,
      ymin = 0,
      ymax = y,
      fill = distribution) + 
  geom_line() +
  geom_ribbon(alpha = 0.5) +
  geom_vline(xintercept = 10) +
  xlab("Time") +
  ylab("Density") +
  labs(fill = "Distribution") +
  xlim(c(0, 30)) +
  scale_fill_manual(values = c("#F8766D", "#00BFC4")) +
  theme_classic() +
  theme(legend.position = 'inside',
        legend.position.inside = c(.8, .8))
```

:::

## Observed time versus true time

:::{.column width="39%"}

Event times are obscured by

- censoring

- end of follow-up, i.e., $h$

Estimate restricted mean survival time (RMST): $E \left[ \text{min}(T, h) \right]$. See @cui2023estimating for more details on adjustment for censoring.

:::

:::{.column width="60%"}

```{r fig.width=4, fig.height=4, out.width='80%'}

dens.obs <- subset(dens.obs, x <= 10)
gg_data <- rbind(dens.cens, dens.obs)

gg_data$distribution <- c(rep("Time to event", nrow(dens.cens)), rep("Observed time", nrow(dens.obs)))
gg_data$distribution[gg_data$x >= 10] <- "Truncate"
gg_data$distribution <- factor(gg_data$distribution, levels = c("Time to event", "Observed time", "Truncate"))

ggplot(gg_data) +
  aes(x = x, 
      y = y, 
      ymin = 0, 
      ymax = y, 
      fill = distribution,
      alpha = distribution) + 
  geom_line() +
  geom_ribbon(alpha = 0.5) +
  geom_vline(xintercept = 10) +
  xlab("Time") +
  ylab("Density") +
  labs(fill = "Distribution") +
  xlim(c(0, 30)) +
  scale_fill_manual(values = c("#F8766D", "#00BFC4", "#818181")) +
  scale_alpha_discrete(range = c(0.5, 1, 0.5)) +
  annotate(geom = 'text', x = 13, y = 0.05, color = 'black', hjust = -0.1,
           label = 'treat as observed\ncensor at h=10') +
  annotate("segment", x = 13, xend = 10, y = 0.05, yend = 0.05,
           arrow = arrow(type = "closed", length = unit(0.02, "npc"))) +
  theme_classic() +
  guides(alpha = 'none') +
  theme(legend.position = 'inside',
        legend.position.inside = c(.8, .8))

```

:::

## Treatment effects for survival

Two treatment effects can be estimated conditional on $h$.

- RMST
$$\tau(x) = E[\min(T(1), h) - \min(T(0), h) \, | X = x],$$
- Survival probability:
$$\tau(x) = P[T(1) > h \, | X = x] - P[T(0) > h \, | X = x].$$
$T(1)$ and $T(0)$ are treated and untreated event times, respectively.

## Summaries of CATEs

You could compute average treatment effect (ATE) as the mean of CATEs:

$$\hat\tau = \frac{1}{n}\sum_{i=1}^n \hat\tau_i(x)$$
But the augmented inverse probability weighted ATE is better:

$$
\hat \tau_{AIPW} = \frac{1}{n} \sum_{i=1}^{n}\left( \overbrace{\tau(X_i)}^{\text{Initial estimate}} + \overbrace{\frac{W_i - e(X_i)}{e(X_i)[1 - e(X_i)]}}^{\text{debiasing weight}} \cdot \overbrace{\left(Y_i - \mu(X_i, W_i)\right)}^{\text{residual}} \right)
$$

## Summaries of CATEs contd.

For simplicity, re-write the augmented inverse probability ATE as

$$\hat\tau = \frac{1}{n}\sum_{i=1}^n \hat\Gamma_i(x),$$
With a vector of these $\Gamma_i$'s, define:


- Average treatment effect (ATE) = `mean(gamma)`

- Best linear projection (BLP) = `lm(gamma ~ X)`

## Estimating the ATE

First, we'll prepare the data:

```{r, eval=FALSE, echo=TRUE}

# loads packages and R functions
targets::tar_load_globals()

# loads a specific target
tar_load(data_sim_1)
```

Second, coerce data to `grf` format: 

```{r, echo = TRUE}

# helper function for grf data prep 
data_grf <- data_coerce_grf(data_sim_1$values)

# just a view of the X matrix
head(data_grf$X, n = 2)

```

## Fitting the forest

Third, fit the causal survival forest: 

```{r, echo = TRUE}

fit_grf <- causal_survival_forest(
  X = data_grf$X, # covariates
  Y = data_grf$Y, # time to event
  W = data_grf$W, # treatment status
  D = data_grf$D, # event status
  horizon = 3, # 3-year horizon
  # treatment effect will be
  # measured in terms of the
  # restricted mean survival time
  target = 'RMST' 
)

fit_grf

```

## Get $\Gamma$ scores

Remember the $\Gamma_i$'s that provide conditional estimates of $\tau$? Let's get them.

```{r, echo = TRUE}

# pull the augmented CATEs from the fitted grf object
gammas <- get_scores(fit_grf)

```

- With `gammas`, we can compute ATE manually
```{r, echo=TRUE}
mean(gammas) 
```

- Verify this is what the `grf` function gives
```{r, echo=TRUE} 
average_treatment_effect(fit_grf)
```

## Your turn

Open `classwork/04-causal_forests.qmd` and complete Exercise 1

- **Reminder**: Red sticky note for help, green sticky when you finish.

- **Note**: the `data_coerce_grf()` function can save you lots of time.

```{r ex-1}
#| echo: false
# countdown::countdown(minutes = 5, id = "ex-1")
```

## Estimating the BLP

The BLP [@semenova2021debiased]:

- Is estimated by regressing a set of covariates on $\Gamma$.

- Can be estimated for a subset of covariates 

- Can be estimated for a subset of observations.

- Summarizes heterogeneous treatment effects conditional on covariates.

You can estimate BLP manually:

```{r, echo = TRUE}

data_blp <- bind_cols(gamma = gammas, data_grf$X)

fit_blp <- lm(gamma ~ ., data = data_blp)

```

## What happens underneath the `grf` hood

Here's how you can replicate `grf` results:

:::{.column width="49%"}

```{r, echo=TRUE}
lmtest::coeftest(fit_blp, 
                 vcov = sandwich::vcovCL, 
                 type = 'HC3')
```

:::

:::{.column width="49%"}

```{r, echo=TRUE}
best_linear_projection(fit_grf, data_grf$X)
```

:::

## Your turn

Complete Exercise 2

```{r ex-2}
#| echo: false
# countdown::countdown(minutes = 5, id = "ex-1")
```

## Your turn

Complete Exercise 3

```{r ex-3}
#| echo: false
# countdown::countdown(minutes = 5, id = "ex-1")
```

## Rank-Weighted Average Treatment Effect

While ATE and BLP are helpful, they do not tell us the following:

1. How good is a *treatment prioritization rule* at distinguishing sub-populations with different conditional treatment effects?

1. Is there any heterogeneity present in a conditional treatment effect?

The rank-weighted average treatment effect (RATE) answers both of these.

## Treatment prioritization rules

Suppose we have a treatment that benefits some (not all) adults. Who should initiate treatment? A treatment prioritization rule can help:

- high score for those likely to benefit from treatment.

- low score for those likely to have a small/negative benefit from treatment.

Risk prediction models can be a treatment prioritization rule:

- Initiate antihypertensive medication if predicted risk for cardiovascular disease is high.

## How to evaluate treatment prioritization

The basic idea:

1. Chop the population up into subgroups based on the prioritization rule, e.g., by decile of score.

1. Estimate the ATE in each group, separately, and compare to the overall estimated ATE from treating everyone

1. Plot the difference between group-specific ATE and the overall ATE for each of the groups

Example: the Targeting Operator Characteristic (TOC)

## Targeting Operator Characteristic (TOC)

:::{.column width="49%"}

- Create groups by including the top q$^\text{th}$ fraction of individuals with the largest prioritization score.

- Use many values of $q$ to make the pattern more curve-like

- Motivation: Receiver Operating Characteristic (ROC) curve, a widely used metric for assessing discrimination of predictions.

:::

:::{.column width="49%"}

```{r toc-plot, echo=FALSE, fig.width=5, fig.height=5}
n <- 1000
p <- 1
X <- matrix(rnorm(n * p), n, p)
tau <- X[, 1]

ATE <- mean(tau)
sort.idx <- order(tau, decreasing = TRUE)
TOC <- rep(NA, n)
for (i in 1:n) {
  TOC[i] <- mean(tau[sort.idx[1:i]]) - ATE
}
q <- seq(1/n, 1, by = 1/n)
df <- data.frame(q, TOC)
ypoint <- df[df$q == 0.05, "TOC"]

ggplot(df, aes(x = q, y = TOC)) +
  geom_line() +
  geom_hline(yintercept = 0, lty = 3) +
  annotate(geom = 'text', x = .1, y = ypoint, color = 'black', hjust = -0.1,
           label = 'ATE of top 5% minus overall ATE') +
  annotate("segment", x = .1, xend = .05, y = ypoint, yend = ypoint,
           arrow = arrow(type = "closed", length = unit(0.02, "npc"))) +
  theme_classic() +
  labs(y = "", title = "Targeting Operator Characteristic")
```

:::

## RATE: area underneath TOC

:::{.column width="49%"}

RATE is estimated by taking the area underneath the TOC curve.

$$\textrm{RATE} = \int_0^1 \textrm{TOC}(q) dq .$$

As $\tau(X_i)$ approaches a constant, RATE approaches 0.

:::

:::{.column width="49%"}

```{r toc-plot-autoc, echo=FALSE, fig.width=5, fig.height=5}

n <- 1000
p <- 1
X <- matrix(rnorm(n * p), n, p)
tau <- X[, 1]

ATE <- mean(tau)
sort.idx <- order(tau, decreasing = TRUE)
TOC <- rep(NA, n)
for (i in 1:n) {
  TOC[i] <- mean(tau[sort.idx[1:i]]) - ATE
}
q <- seq(1/n, 1, by = 1/n)
df <- data.frame(q, TOC)

ggplot(df, aes(x = q, y = TOC)) +
  geom_line() +
  geom_hline(yintercept = 0, lty = 3) +
  theme_classic() +
  labs(y = "", title = "Targeting Operator Characteristic") +
  geom_ribbon(aes(ymin = 0), ymax=TOC, alpha = 0.5, fill = "red") +
  annotate(geom = 'text', x = 0.15, y = 0.6, color = 'black', hjust = -0.1,
           label = "RATE")

```

:::

## RATE of prediction model

Let's use RATE to see how well risk prediction works as a treatment prioritization rule.

```{r, echo=TRUE}

library(aorsf)

fit_orsf <- orsf(time + status ~ .,
                 data = data_sim_1$values,
                 oobag_pred_horizon = 3)

# important to use oobag!
prd_risk <- as.numeric(fit_orsf$pred_oobag)

prd_rate <- 
  rank_average_treatment_effect(fit_grf, 
                                priorities = prd_risk)

```

## Not good

```{r, fig.height=5, fig.width=8, out.width='100%'}
plot(prd_rate, 
     xlab = "Treated fraction", 
     main = paste("TOC of predicted risk at horizon of",
                  fit_orsf$pred_horizon))
```

## Estimating RATE from CATE

An intuitive way to assign treatment priority is to use the CATE: $\hat\tau(X_i)$

- $\hat\tau(X_i)$ should **not** be estimated and evaluated using the same data

- Use split-sample estimation or cross-fitting [@yadlowsky2021evaluating].

As a preliminary step, we'll split our data in to training and testing sets

```{r, echo=TRUE}

train_index <- sample(x = nrow(data_sim_1$values), size = 1250)

data_trn <- data_sim_1$values[train_index, ]
data_tst <- data_sim_1$values[-train_index, ]

data_trn_grf <- data_coerce_grf(data_trn)
data_tst_grf <- data_coerce_grf(data_tst)

```


---

## Fitting

We fit one forest with training data to estimate CATE and fit another forest with testing data to evaluate the CATE estimates:

:::{.column width="49%"}

```{r, echo=TRUE}
fit_trn_grf <- causal_survival_forest(
  X = data_trn_grf$X, Y = data_trn_grf$Y,
  W = data_trn_grf$W, D = data_trn_grf$D,
  horizon = 3, target = 'RMST' 
)

fit_tst_grf <- causal_survival_forest(
  X = data_tst_grf$X, Y = data_tst_grf$Y,
  W = data_tst_grf$W, D = data_tst_grf$D,
  horizon = 3, target = 'RMST' 
)
```


:::

:::{.column width="49%"}

![](diagram/rate-1.svg)
:::

## Predicting

We use the forest fitted to training data to estimate CATE for the testing data. For illustration, we also estimate naive CATE

:::{.column width="49%"}

```{r, echo=TRUE}

# the fitted forest hasn't
# seen the testing data
tau_hat_split <- fit_trn_grf %>% 
  predict(data_tst_grf$X) %>% 
  getElement("predictions")

# Illustration only (don't do this)
tau_hat_naive <- fit_trn_grf %>% 
  predict(data_trn_grf$X) %>% 
  getElement("predictions")
```

:::

:::{.column width="49%"}

![](diagram/rate-2.svg)
:::

## Evaluating

We use the forest fitted to the testing data to evaluate the CATE estimates for observations in the testing data

:::{.column width="49%"}

```{r, echo=TRUE}


rate_split <- 
  rank_average_treatment_effect(
    forest = fit_tst_grf, 
    priorities = tau_hat_split, 
    target = "AUTOC"
  )

# Illustration only (don't do this)
rate_naive <- 
  rank_average_treatment_effect(
    forest = fit_trn_grf, 
    priorities = tau_hat_naive, 
    target = "AUTOC"
  )

```

:::

:::{.column width="49%"}

![](diagram/rate-full.svg)
:::

## Correct versus overly optimistic

The problem with being overly optimistic is it has very high type 1 error

```{r, fig.width=11, fig.height=5}

data_gg <- bind_rows(
  `Correct split sample test` = rate_split$TOC,
  `Naive double dip` = rate_naive$TOC,
  .id = 'type'
)

ggplot(data_gg) + 
  aes(x = q, 
      y = estimate, 
      ymin = estimate - 1.96*std.err, 
      ymax = estimate + 1.96*std.err) +
  geom_ribbon(alpha = 0.2) + 
  geom_line() + 
  facet_wrap(~type) + 
  theme_bw() + 
  theme(panel.grid = element_blank(), 
        text = element_text(size=15)) + 
  geom_hline(yintercept = 0, linetype = 2, alpha = 0.2) + 
  labs(x = "Treated fraction (q)",
       y = "ATE at q - overall ATE")

```

## Your turn

Complete Exercise 4

```{r ex-4}
#| echo: false
# countdown::countdown(minutes = 5, id = "ex-1")
```

## References

